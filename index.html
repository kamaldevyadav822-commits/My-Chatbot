<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>AI Chatbot</title>
  <style>
    body{font-family:Arial,Helvetica,sans-serif;background:#f4f6f8;margin:0}
    .wrap{max-width:760px;margin:28px auto;background:#fff;padding:18px;border-radius:12px;box-shadow:0 6px 20px rgba(0,0,0,0.06)}
    h1{text-align:center;margin:0 0 12px;font-size:28px}
    #chat{height:360px;overflow:auto;background:#fafafa;padding:12px;border-radius:8px;border:1px solid #eee}
    .msg{margin:8px 0;padding:10px;border-radius:8px;max-width:86%;word-wrap:break-word}
    .user{background:#dbeeff;color:#003; margin-left:auto;text-align:right}
    .bot{background:#eef7e9;color:#044; margin-right:auto;text-align:left}
    .system{background:#f0f0f0;color:#444;text-align:center}
    .controls{display:flex;gap:8px;margin-top:12px}
    input[type="text"]{flex:1;padding:10px;border-radius:8px;border:1px solid #ddd}
    button{padding:10px 12px;border-radius:8px;border:none;background:#0b3b76;color:#fff;cursor:pointer}
    button.secondary{background:#333}
    button:disabled{opacity:.5;cursor:default}
    #status{margin-top:10px;color:#666;font-size:13px;text-align:center}
    #debug{font-family:monospace;font-size:12px;margin-top:14px;background:#071029;color:#cfe7ff;padding:8px;border-radius:6px;max-height:160px;overflow:auto;white-space:pre-wrap}
    #remoteAudio{display:none}
    .row { display:flex; gap:8px; align-items:center; }
  </style>
</head>
<body>
  <div class="wrap">
    <h1>AI Chatbot</h1>

    <div id="chat" aria-live="polite"></div>

    <div class="controls">
      <input id="textInput" placeholder="Type a message..." />
      <button id="sendBtn">Send</button>
      <button id="micBtn" class="secondary">ðŸŽ¤ Mic</button>
      <button id="connectBtn" class="secondary">Connect</button>
    </div>

    <div id="status">Status: idle</div>
    <pre id="debug"></pre>

    <audio id="remoteAudio" autoplay playsinline></audio>
  </div>

<script>
(() => {
  // -------------------------
  // Config / paths
  // -------------------------
  const ORIGIN = window.location.origin;        // same origin (served by your Render backend)
  const SESSION_PATH = ORIGIN + '/session';     // expects GET /session (server creates ephemeral realtime session)
  const CHAT_PATH = ORIGIN + '/chat';           // expects POST /chat (server-side chat fallback)
  // -------------------------

  // UI elements
  const chatEl = document.getElementById('chat');
  const statusEl = document.getElementById('status');
  const debugEl = document.getElementById('debug');
  const inputEl = document.getElementById('textInput');
  const sendBtn = document.getElementById('sendBtn');
  const connectBtn = document.getElementById('connectBtn');
  const micBtn = document.getElementById('micBtn');
  const audioEl = document.getElementById('remoteAudio');

  // RTC + state
  let pc = null;
  let dc = null;
  let localStream = null;
  let token = null;
  let model = null;
  let dataChannelOpen = false;
  let currentAssistantEl = null; // for streaming deltas
  let queuedMessages = [];

  // helpers
  function logDebug(...args){
    console.log(...args);
    debugEl.textContent += args.map(a => typeof a === 'string'? a : JSON.stringify(a)).join(' ') + '\\n';
    debugEl.scrollTop = debugEl.scrollHeight;
  }
  function setStatus(s){
    statusEl.textContent = 'Status: ' + s;
  }
  function appendMsg(role, text){
    const d = document.createElement('div');
    d.className = 'msg ' + (role === 'user'? 'user' : role === 'assistant' ? 'bot' : 'system');
    d.textContent = text;
    chatEl.appendChild(d);
    chatEl.scrollTop = chatEl.scrollHeight;
    return d;
  }
  function appendSystem(t){ appendMsg('system', t); }

  // -------------------------
  // sendText: uses realtime if connected, otherwise /chat fallback
  // -------------------------
  async function sendText(){
    const text = inputEl.value.trim();
    if(!text) return;
    appendMsg('user', text);
    inputEl.value = '';

    // If realtime data channel is open, send there (streams & TTS)
    if(dc && dataChannelOpen){
      const ev = { type: 'response.create', response: { instructions: text } };
      try {
        dc.send(JSON.stringify(ev));
        logDebug('Sent via realtime data channel', ev);
        // show a streaming assistant element to be filled by deltas
        currentAssistantEl = appendMsg('assistant', '');
      } catch (err){
        logDebug('Realtime send error', err);
        appendSystem('Realtime send failed â€” falling back to /chat');
        await sendViaChatAPI(text); // fallback
      }
      return;
    }

    // Fallback to server-side /chat
    await sendViaChatAPI(text);
  }

  async function sendViaChatAPI(text){
    try {
      setStatus('Sending to /chat...');
      const r = await fetch(CHAT_PATH, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ message: text })
      });
      if(!r.ok){
        const txt = await r.text().catch(()=>null);
        setStatus('Error from /chat: ' + r.status);
        logDebug('/chat response not ok', r.status, txt);
        appendSystem('âš ï¸ No reply (server error)');
        return;
      }
      const data = await r.json();
      const reply = data?.reply ?? 'âš ï¸ No reply';
      appendMsg('assistant', reply);
      setStatus('Idle');
    } catch (err) {
      logDebug('/chat fetch error', err);
      appendSystem('âŒ Error: Unable to connect to /chat');
      setStatus('Idle');
    }
  }

  // -------------------------
  // Realtime connect flow (GET /session -> OpenAI /v1/realtime SDP exchange)
  // -------------------------
  async function connectRealtime(){
    if(pc){
      appendSystem('Already connected or connecting');
      return;
    }
    setStatus('Requesting ephemeral session...');
    appendSystem('Requesting ephemeral session token from server...');

    try {
      const resp = await fetch(SESSION_PATH); // your server must expose GET /session
      if(!resp.ok){
        const txt = await resp.text().catch(()=>null);
        throw new Error('Session fetch failed: ' + resp.status + ' ' + txt);
      }
      const session = await resp.json();
      // session shape may vary; try common fields
      token = session?.client_secret?.value ?? session?.client_secret ?? session?.client_secret?.value ?? null;
      model = session?.model ?? session?.model ?? 'gpt-4o-realtime-preview-2024-12-17';
      logDebug('session', session);
      if(!token) throw new Error('No ephemeral token returned by server');
    } catch (err){
      logDebug('session error', err);
      appendSystem('âŒ Failed to get session token. Check server logs.');
      setStatus('Session error');
      return;
    }

    setStatus('Creating WebRTC PeerConnection...');
    pc = new RTCPeerConnection();

    pc.ontrack = (e) => {
      logDebug('ontrack received', e);
      audioEl.srcObject = e.streams[0];
      audioEl.style.display = 'block';
    };
    pc.onicecandidate = e => {
      // we don't need to send candidates â€” the OpenAI endpoint handles ICE in the SDP exchange
      if(e.candidate) logDebug('ICE candidate', e.candidate);
    };

    // create data channel
    dc = pc.createDataChannel('oai-events');
    dc.onopen = () => {
      dataChannelOpen = true;
      setStatus('Realtime connected â€” data channel open');
      logDebug('data channel open');
      // flush queued messages (if user clicked send before open)
      while(queuedMessages.length){
        const m = queuedMessages.shift();
        dc.send(m);
      }
    };
    dc.onclose = () => {
      dataChannelOpen = false;
      setStatus('Realtime data channel closed');
      logDebug('data channel closed');
    };
    dc.onerror = (e) => logDebug('data channel error', e);
    dc.onmessage = (ev) => {
      try { handleRealtimeEvent(ev.data); } catch(e){ logDebug('handleRealtimeEvent error', e); }
    };

    // receive audio from model
    pc.addTransceiver('audio', { direction: 'recvonly' });

    try {
      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);

      const realtimeUrl = `https://api.openai.com/v1/realtime?model=${encodeURIComponent(model)}`;
      logDebug('Posting SDP offer to OpenAI realtime', realtimeUrl);
      const sdpResp = await fetch(realtimeUrl, {
        method: 'POST',
        headers: {
          Authorization: 'Bearer ' + token,
          'Content-Type': 'application/sdp'
        },
        body: offer.sdp
      });

      if(!sdpResp.ok){
        const txt = await sdpResp.text().catch(()=>null);
        throw new Error('OpenAI SDP POST failed: ' + sdpResp.status + ' ' + txt);
      }

      const answer = await sdpResp.text();
      await pc.setRemoteDescription({ type: 'answer', sdp: answer });
      setStatus('Realtime connected (WebRTC established)');
      appendSystem('Realtime connected. You can send messages; audio (TTS) will play if model sends audio track.');
    } catch (err){
      logDebug('realtime connect error', err);
      appendSystem('âŒ Realtime connect failed. Check server logs and that token is valid.');
      setStatus('Realtime connect failed');
      // cleanup partial
      try { pc.close(); } catch {}
      pc = null;
      dc = null;
    }
  }

  // -------------------------
  // Parse and display incoming realtime events
  // -------------------------
  function handleRealtimeEvent(raw){
    logDebug('realtime raw:', raw);
    let msg;
    try { msg = JSON.parse(raw); } catch(e) {
      // plain text fallback
      appendMsg('assistant', raw);
      return;
    }
    // If streaming delta -> append/update current assistant element
    const text = extractTextFromEvent(msg);
    if(text != null){
      // If response.delta or partials, update currentAssistantEl
      if(currentAssistantEl){
        currentAssistantEl.textContent += text;
      } else {
        currentAssistantEl = appendMsg('assistant', text);
      }
    }

    // If response.completed -> finalize and clear currentAssistantEl
    if(msg.type === 'response.completed'){
      currentAssistantEl = null;
    }
  }

  function extractTextFromEvent(obj){
    // Known shapes:
    // { type: "response.output_text.delta", delta: "..." }
    // { type: "response.delta", delta: "..." }
    // { type: "response.completed", response: { output_text: "final" } }
    if(!obj) return null;
    if(typeof obj === 'string') return obj;
    if(typeof obj.delta === 'string') return obj.delta;
    if(obj.type && obj.type.toString().includes('output_text') && typeof obj.delta === 'string') return obj.delta;
    if(obj.type === 'response.completed'){
      // try to find final text
      try {
        if(typeof obj.response?.output_text === 'string') return obj.response.output_text;
        if(typeof obj.response?.output === 'object') {
          // attempt to dig into response.output array structure
          const out = obj.response.output;
          for(const item of out){
            if(item?.content?.[0]?.text) return item.content[0].text;
          }
        }
      } catch(e){}
    }
    // fallback: first string leaf
    return findFirstString(obj);
  }
  function findFirstString(o, seen=new Set()){
    if(!o) return null;
    if(typeof o === 'string' && o.trim() !== '') return o;
    if(typeof o !== 'object') return null;
    if(seen.has(o)) return null;
    seen.add(o);
    for(const k of Object.keys(o)){
      const v = o[k];
      if(typeof v === 'string' && v.trim() !== '') return v;
      const nested = findFirstString(v, seen);
      if(nested) return nested;
    }
    return null;
  }

  // -------------------------
  // Microphone toggling (adds tracks to peer connection)
  // -------------------------
  async function toggleMic(){
    if(localStream){
      localStream.getTracks().forEach(t => t.stop());
      localStream = null;
      micBtn.textContent = 'ðŸŽ¤ Mic';
      appendSystem('Microphone stopped');
      return;
    }
    try {
      localStream = await navigator.mediaDevices.getUserMedia({ audio: true });
      if(!pc){
        appendSystem('Start Realtime (Connect) before enabling mic');
        // stop tracks we just created
        localStream.getTracks().forEach(t => t.stop());
        localStream = null;
        return;
      }
      localStream.getTracks().forEach(track => pc.addTrack(track, localStream));
      micBtn.textContent = 'ðŸ”´ Recording';
      appendSystem('Microphone enabled and added to connection');
    } catch (err){
      appendSystem('Microphone permission denied or error');
      logDebug('getUserMedia error', err);
    }
  }

  // -------------------------
  // Event bindings
  // -------------------------
  sendBtn.addEventListener('click', sendText);
  inputEl.addEventListener('keydown', (e) => { if(e.key === 'Enter') sendText(); });
  connectBtn.addEventListener('click', connectRealtime);
  micBtn.addEventListener('click', toggleMic);

  // initial UI
  setStatus('idle â€” use Connect for realtime or type to use /chat fallback');
  appendSystem('Page ready. Type and press Send (will use /chat). Or press Connect to enable realtime voice and streaming replies.');
  logDebug('Client initialized. SERVER paths:', { SESSION_PATH, CHAT_PATH });

})();
</script>
</body>
</html>
